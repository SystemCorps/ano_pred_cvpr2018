{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from models import generator, discriminator, flownet, initialize_flownet\n",
    "from loss_functions import intensity_loss, gradient_loss\n",
    "from v_utils import DataLoader, load, save, psnr_error\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'vessel'\n",
    "train_folder = '/dmount/Vessel/drawing_anno/Patient/train/image'\n",
    "train_gtfolder = '/dmount/Vessel/drawing_anno/Patient/train/label'\n",
    "test_folder = '/dmount/Vessel/drawing_anno/Patient/vessel/30_1_in'\n",
    "test_gtfolder = '/dmount/Vessel/drawing_anno/Patient/vessel/30_1_out'\n",
    "\n",
    "train_dirs = glob(os.path.join(train_folder, '*.jpg'))\n",
    "train_gtdirs = glob(os.path.join(train_gtfolder, '*.jpg'))\n",
    "test_dirs = glob(os.path.join(test_folder, '*.jpg'))\n",
    "test_gtdirs = glob(os.path.join(test_gtfolder, '*.jpg'))\n",
    "\n",
    "train_dirs.sort()\n",
    "train_gtdirs.sort()\n",
    "test_dirs.sort()\n",
    "test_gtdirs.sort()\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 500\n",
    "iterations = (len(train_dirs)//8 + 1) * epochs\n",
    "height, width = 256, 256\n",
    "\n",
    "\n",
    "l_num = 2\n",
    "alpha_num = 1\n",
    "lam_lp = 1.0\n",
    "lam_gdl = 1.0\n",
    "\n",
    "trial = 3\n",
    "\n",
    "\n",
    "summary_dir = 'v_summary/trial_{}'.format(trial)\n",
    "if not os.path.exists(summary_dir):\n",
    "    os.makedirs(summary_dir)\n",
    "\n",
    "snapshot_dir = 'v_snapshot/trial_{}'.format(trial)\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.makedirs(snapshot_dir)\n",
    "\n",
    "\n",
    "\n",
    "lr_bounds = [7000]\n",
    "lr = [0.0001, 1e-05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inimg = np.zeros((len(train_dirs), height, width, 3), dtype=np.float32)\n",
    "train_gtimg = np.zeros((len(train_dirs), height, width, 3), dtype=np.float32)\n",
    "test_inimg = np.zeros((len(test_dirs), height, width, 3), dtype=np.float32)\n",
    "test_gtimg = np.zeros((len(test_dirs), height, width, 3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_inimg)):\n",
    "    img = cv2.imread(train_dirs[i])\n",
    "    img = cv2.resize(img, (height, width))\n",
    "    train_inimg[i] = (img / 127.5) - 1.0\n",
    "    \n",
    "    img = cv2.imread(train_gtdirs[i])\n",
    "    img = cv2.resize(img, (height, width))\n",
    "    train_gtimg[i] = (img / 127.5) - 1.0\n",
    "    \n",
    "for i in range(len(test_inimg)):\n",
    "    img = cv2.imread(test_dirs[i])\n",
    "    img = cv2.resize(img, (height, width))\n",
    "    test_inimg[i] = (img / 127.5) - 1.0\n",
    "    \n",
    "    img = cv2.imread(test_gtdirs[i])\n",
    "    img = cv2.resize(img, (height, width))\n",
    "    test_gtimg[i] = (img / 127.5) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset\n",
    "with tf.name_scope('dataset'):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_inimg).repeat().batch(batch_size)\n",
    "    train_gtdataset = tf.data.Dataset.from_tensor_slices(train_gtimg).repeat().batch(batch_size)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_inimg).repeat().batch(batch_size)\n",
    "    test_gtdataset = tf.data.Dataset.from_tensor_slices(test_gtimg).repeat().batch(batch_size)\n",
    "    \n",
    "    \"\"\"train_dataset = tf.data.Dataset.from_tensor_slices(train_inimg).repeat(epochs)\n",
    "    train_gtdataset = tf.data.Dataset.from_tensor_slices(train_gtimg).repeat(epochs)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_inimg).repeat(epochs)\n",
    "    test_gtdataset = tf.data.Dataset.from_tensor_slices(test_gtimg).repeat(epochs)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"train_dataset = tf.data.Dataset.from_tensor_slices(train_inimg)\n",
    "    train_dataset.repeat()\n",
    "    train_dataset.batch(batch_size)\n",
    "    \n",
    "    train_gtdataset = tf.data.Dataset.from_tensor_slices(train_gtimg)\n",
    "    train_gtdataset.repeat()\n",
    "    train_gtdataset.batch(batch_size)\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_inimg)\n",
    "    test_dataset.repeat()\n",
    "    test_dataset.batch(batch_size)\n",
    "    \n",
    "    test_gtdataset = tf.data.Dataset.from_tensor_slices(test_gtimg)\n",
    "    test_gtdataset.repeat()\n",
    "    test_gtdataset.batch(batch_size)\"\"\"\n",
    "    \n",
    "    \n",
    "    train_it = train_dataset.make_one_shot_iterator()\n",
    "    train_gtit = train_gtdataset.make_one_shot_iterator()\n",
    "\n",
    "    test_it = test_dataset.make_one_shot_iterator()\n",
    "    test_gtit = test_gtdataset.make_one_shot_iterator()\n",
    "\n",
    "\n",
    "    train_inputs = train_it.get_next()\n",
    "    train_gt = train_gtit.get_next()\n",
    "\n",
    "    test_inputs = test_it.get_next()\n",
    "    test_gt = test_gtit.get_next()\n",
    "    \n",
    "\n",
    "# define training generator function\n",
    "with tf.variable_scope('generator', reuse=None):\n",
    "    print('training = {}'.format(tf.get_variable_scope().name))\n",
    "    train_outputs = generator(train_inputs, layers=4, output_channel=3)\n",
    "    train_psnr_error = psnr_error(gen_frames=train_outputs, gt_frames=train_gt)\n",
    "\n",
    "# define testing generator function\n",
    "with tf.variable_scope('generator', reuse=True):\n",
    "    print('testing = {}'.format(tf.get_variable_scope().name))\n",
    "    test_outputs = generator(test_inputs, layers=4, output_channel=3)\n",
    "    test_psnr_error = psnr_error(gen_frames=test_outputs, gt_frames=test_gt)\n",
    "\n",
    "\n",
    "# define intensity loss\n",
    "if lam_lp != 0:\n",
    "    lp_loss = intensity_loss(gen_frames=train_outputs, gt_frames=train_gt, l_num=l_num)\n",
    "else:\n",
    "    lp_loss = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# define gdl loss\n",
    "if lam_gdl != 0:\n",
    "    gdl_loss = gradient_loss(gen_frames=train_outputs, gt_frames=train_gt, alpha=alpha_num)\n",
    "else:\n",
    "    gdl_loss = tf.constant(0.0, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('training'):\n",
    "    g_loss = tf.add_n([lp_loss * lam_lp, gdl_loss * lam_gdl], name='g_loss')\n",
    "\n",
    "    g_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='g_step')\n",
    "    g_lrate = tf.train.piecewise_constant(g_step, boundaries=lr_bounds, values=lr)\n",
    "    g_optimizer = tf.train.AdamOptimizer(learning_rate=g_lrate, name='g_optimizer')\n",
    "    g_vars = tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "\n",
    "    g_train_op = g_optimizer.minimize(g_loss, global_step=g_step, var_list=g_vars, name='g_train_op')\n",
    "\n",
    "\n",
    "# add all to summaries\n",
    "tf.summary.scalar(tensor=train_psnr_error, name='train_psnr_error')\n",
    "tf.summary.scalar(tensor=test_psnr_error, name='test_psnr_error')\n",
    "tf.summary.scalar(tensor=g_loss, name='g_loss')\n",
    "tf.summary.scalar(tensor=lp_loss, name='intensity_loss')\n",
    "tf.summary.scalar(tensor=gdl_loss, name='gradient_loss')\n",
    "tf.summary.image(tensor=train_outputs, name='train_outputs')\n",
    "tf.summary.image(tensor=train_gt, name='train_gt')\n",
    "tf.summary.image(tensor=test_outputs, name='test_outputs')\n",
    "tf.summary.image(tensor=test_gt, name='test_gt')\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    # summaries\n",
    "    summary_writer = tf.summary.FileWriter(summary_dir, graph=sess.graph)\n",
    "\n",
    "    # initialize weights\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Init successfully!')\n",
    "\n",
    "\n",
    "    # tf saver\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=None)\n",
    "    restore_var = [v for v in tf.global_variables()]\n",
    "    loader = tf.train.Saver(var_list=restore_var)\n",
    "    if os.path.isdir(snapshot_dir):\n",
    "        ckpt = tf.train.get_checkpoint_state(snapshot_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            load(loader, sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print('No checkpoint file found.')\n",
    "    else:\n",
    "        load(loader, sess, snapshot_dir)\n",
    "\n",
    "    _step, _loss, _summaries = 0, None, None\n",
    "    \n",
    "    while _step < iterations:\n",
    "        print('Training generator...')\n",
    "        _, _g_lr, _step, _lp_loss, _gdl_loss, _g_loss, _train_psnr, _summaries = sess.run(\n",
    "            [g_train_op, g_lrate, g_step, lp_loss, gdl_loss, g_loss, train_psnr_error, summary_op])\n",
    "\n",
    "        if _step % 10 == 0:\n",
    "            print('GeneratorModel : Step {}, lr = {:.6f}'.format(_step, _g_lr))\n",
    "            print('                 Global      Loss : ', _g_loss)\n",
    "            print('                 intensity   Loss : ({:.4f} * {:.4f} = {:.4f})'.format(_lp_loss, lam_lp, _lp_loss * lam_lp))\n",
    "            print('                 gradient    Loss : ({:.4f} * {:.4f} = {:.4f})'.format( _gdl_loss, lam_gdl, _gdl_loss * lam_gdl))\n",
    "            print('                 PSNR  Error      : ', _train_psnr)\n",
    "        \n",
    "        if _step % 1000 == 0:\n",
    "            summary_writer.add_summary(_summaries, global_step=_step)\n",
    "            print('Save summaries...')\n",
    "\n",
    "        if _step % 5000 == 0:\n",
    "            save(saver, sess, snapshot_dir, _step)\n",
    "\n",
    "            \n",
    "    print('Finish successfully!')\n",
    "    save(saver, sess, snapshot_dir, _step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
